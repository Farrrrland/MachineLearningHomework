{"metadata":{"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchtext\nimport torch.nn as nn\nfrom torchtext.legacy import data\nfrom torchtext.legacy import datasets\nimport torch.optim as optim\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTEXT = data.Field(include_lengths=True)\n\n# If you want to use English tokenizer from SpaCy, you need to install SpaCy and download its English model:\n# pip install spacy\n# python -m spacy download en_core_web_sm\n# TEXT = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n\nLABEL = data.LabelField(dtype=torch.long)\ntrain_data, valid_data, test_data = datasets.SST.splits(TEXT, LABEL, train_subtrees=True, filter_pred=lambda ex: ex.label != 'neutral')\n\nTEXT.build_vocab(train_data)\n# Here, you can also use some pre-trained embedding\n# TEXT.build_vocab(train_data,\n#                  vectors=\"glove.6B.100d\",\n#                  unk_init=torch.Tensor.normal_)\nLABEL.build_vocab(train_data)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 64\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    sort_key=lambda x: len(x.text),\n    batch_size=batch_size, device=device)\nprint(\"here\")","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"here\n","output_type":"stream"}],"id":"86d7520b-9b05-4963-8926-551dce7dfa6e"},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"tags":[],"jp-MarkdownHeadingCollapsed":true},"id":"9fe9a5e2-872c-4e1d-9cd5-1c0d1ae485d0"},{"cell_type":"code","source":"print(len(TEXT.vocab.itos))","metadata":{"tags":[],"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"18003\n","output_type":"stream"}],"id":"d7e9e542-4489-4650-9dce-0572a5ee5090"},{"cell_type":"code","source":"for batch in train_iterator.batches:\n    print(len(batch))\n    # print(batch)\n    print('LABEL\\tLENGTH\\tTEXT'.ljust(10))\n    for example in batch:\n        print('%s\\t%d\\t%s'.ljust(10) % (example.label, len(example.text), example.text))\n        # print('\\n')\n    break","metadata":{"tags":[],"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"64\nLABEL\tLENGTH\tTEXT\npositive\t3\t['extremely', 'talented', 'musicians']  \npositive\t5\t['will', 'definitely', 'win', 'some', 'hearts']  \npositive\t1\t['Divine']  \nnegative\t10\t[\"'re\", 'not', 'big', 'fans', 'of', 'teen', 'pop', 'kitten', 'Britney', 'Spears']  \npositive\t10\t['delighted', 'simply', 'to', 'spend', 'more', 'time', 'with', 'familiar', 'cartoon', 'characters']  \npositive\t10\t['Hollywood', 'has', 'crafted', 'a', 'solid', 'formula', 'for', 'successful', 'animated', 'movies']  \nnegative\t1\t['destruction']  \nnegative\t12\t['it', 'was', 'co-written', 'by', 'Mattel', 'executives', 'and', 'lobbyists', 'for', 'the', 'tinsel', 'industry']  \nnegative\t1\t['-LRB-']  \nnegative\t3\t['a', 'bad', 'plot']  \nnegative\t18\t['Mark', 'Wahlberg', 'and', 'Thandie', 'Newton', 'are', 'not', 'Hepburn', 'and', 'Grant', ',', 'two', 'cinematic', 'icons', 'with', 'chemistry', 'galore', '.']  \npositive\t1\t['succeeds']  \nnegative\t1\t['cannibal']  \nnegative\t12\t['have', 'benefited', 'from', 'a', 'little', 'more', 'dramatic', 'tension', 'and', 'some', 'more', 'editing']  \npositive\t20\t['good', 'news', 'to', 'anyone', 'who', \"'s\", 'fallen', 'under', 'the', 'sweet', ',', 'melancholy', 'spell', 'of', 'this', 'unique', 'director', \"'s\", 'previous', 'films']  \npositive\t8\t['eventually', 'begins', 'to', 'yield', 'some', 'interesting', 'results', '.']  \npositive\t1\t['best']  \nnegative\t15\t['cause', 'loads', 'of', 'irreparable', 'damage', 'that', 'years', 'and', 'years', 'of', 'costly', 'analysis', 'could', 'never', 'fix']  \nnegative\t8\t['something', 'is', 'rotten', 'in', 'the', 'state', 'of', 'California']  \nnegative\t1\t['off']  \nnegative\t7\t[',', 'it', 'still', 'feels', 'somewhat', 'unfinished', '.']  \nnegative\t3\t['it', 'goes', 'nowhere']  \nnegative\t24\t['...', 'a', 'sour', 'little', 'movie', 'at', 'its', 'core', ';', 'an', 'exploration', 'of', 'the', 'emptiness', 'that', 'underlay', 'the', 'relentless', 'gaiety', 'of', 'the', '1920', \"'s\", '...']  \nnegative\t9\t['little', 'to', 'be', 'learned', 'from', 'watching', '`', 'Comedian', \"'\"]  \npositive\t1\t['elegant']  \npositive\t3\t['recommend', 'the', 'film']  \npositive\t6\t['The', 'movie', 'is', 'gorgeously', 'made', ',']  \nnegative\t2\t['the', 'hype']  \nnegative\t31\t['has', 'about', '3\\\\/4th', 'the', 'fun', 'of', 'its', 'spry', '2001', 'predecessor', '--', 'but', 'it', \"'s\", 'a', 'rushed', ',', 'slapdash', ',', 'sequel-for-the-sake', '-', 'of-a-sequel', 'with', 'less', 'than', 'half', 'the', 'plot', 'and', 'ingenuity', '.']  \npositive\t24\t['she', 'looks', 'like', 'the', 'six-time', 'winner', 'of', 'the', 'Miss', 'Hawaiian', 'Tropic', 'Pageant', ',', 'so', 'I', 'do', \"n't\", 'know', 'what', 'she', \"'s\", 'doing', 'in', 'here']  \nnegative\t2\t['painfully', 'bad']  \npositive\t1\t['convincing']  \nnegative\t1\t['not']  \npositive\t1\t['ambiguous']  \nnegative\t4\t['failing', ',', 'ultimately', ',']  \npositive\t2\t['a', 'masterpiece']  \nnegative\t2\t['propaganda', 'film']  \nnegative\t1\t['lame']  \npositive\t6\t['Everything', 'that', 'was', 'right', 'about', 'Blade']  \npositive\t6\t['A', 'meditation', 'on', 'faith', 'and', 'madness']  \npositive\t15\t['If', 'the', 'real-life', 'story', 'is', 'genuinely', 'inspirational', ',', 'the', 'movie', 'stirs', 'us', 'as', 'well', '.']  \npositive\t5\t['the', 'director', 'is', 'a', 'magician']  \npositive\t1\t['traditional']  \npositive\t1\t['alive']  \nnegative\t1\t['least']  \npositive\t8\t['A', 'funny', 'and', 'touching', 'film', 'that', 'is', 'gorgeously']  \npositive\t4\t[',', 'wall-to-wall', 'good', 'time']  \npositive\t6\t['is', 'worth', 'your', 'time', ',', 'especially']  \nnegative\t24\t[\"'s\", 'a', 'boring', 'movie', 'about', 'a', 'boring', 'man', ',', 'made', 'watchable', 'by', 'a', 'bravura', 'performance', 'from', 'a', 'consummate', 'actor', 'incapable', 'of', 'being', 'boring', '.']  \nnegative\t1\t['Fails']  \npositive\t4\t['while', 'cleverly', 'worked', 'out']  \npositive\t21\t[\"'s\", 'good', ',', 'hard-edged', 'stuff', ',', 'violent', 'and', 'a', 'bit', 'exploitative', 'but', 'also', 'nicely', 'done', ',', 'morally', 'alert', 'and', 'street-smart', '.']  \npositive\t9\t['The', 'author', \"'s\", 'devotees', 'will', 'probably', 'find', 'it', 'fascinating']  \nnegative\t7\t['imagine', 'acting', 'that', 'could', 'be', 'any', 'flatter']  \npositive\t11\t['The', 'performances', 'are', 'immaculate', ',', 'with', 'Roussillon', 'providing', 'comic', 'relief', '.']  \npositive\t3\t['the', 'light-footed', 'enchantment']  \nnegative\t1\t['misguided']  \npositive\t10\t['-LRB-', 'At', 'least', '-RRB-', 'Moore', 'is', 'a', 'real', 'charmer', '.']  \npositive\t11\t['every', 'member', 'of', 'the', 'ensemble', 'has', 'something', 'fascinating', 'to', 'do', '--']  \nnegative\t2\t['hastily', 'dubbed']  \nnegative\t21\t[\"'s\", 'such', 'a', 'mechanical', 'endeavor', '-LRB-', 'that', '-RRB-', 'it', 'never', 'bothers', 'to', 'question', 'why', 'somebody', 'might', 'devote', 'time', 'to', 'see', 'it']  \npositive\t14\t['is', 'the', 'distinct', 'and', 'very', 'welcome', 'sense', 'of', 'watching', 'intelligent', 'people', 'making', 'a', 'movie']  \npositive\t42\t['has', 'a', 'dashing', 'and', 'resourceful', 'hero', ';', 'a', 'lisping', ',', 'reptilian', 'villain', ';', 'big', 'fights', ';', 'big', 'hair', ';', 'lavish', 'period', 'scenery', ';', 'and', 'a', 'story', 'just', 'complicated', 'enough', 'to', 'let', 'you', 'bask', 'in', 'your', 'own', 'cleverness', 'as', 'you', 'figure', 'it', 'out']  \npositive\t14\t['The', 'one-liners', 'are', 'snappy', ',', 'the', 'situations', 'volatile', 'and', 'the', 'comic', 'opportunities', 'richly', 'rewarded']  \n","output_type":"stream"}],"id":"68a35a81-617f-4c50-a12f-10bed022bb47"},{"cell_type":"code","source":"for i, ((encode, length), labels) in enumerate(train_iterator):\n    tencode = encode.transpose(0,1)\n    print(tencode)\n    print(len(tencode))\n    print(encode)\n    print(length)\n    print(labels)\n    print(\"-----------------------------------------------------\")\n    break","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor([[  11,    3,  327,  ...,    1,    1,    1],\n        [ 238,    1,    1,  ...,    1,    1,    1],\n        [   9,   28,    4,  ...,    1,    1,    1],\n        ...,\n        [  56,    1,    1,  ...,    1,    1,    1],\n        [  48,    1,    1,  ...,    1,    1,    1],\n        [  14, 5033,   14,  ...,    1,    1,    1]])\n64\ntensor([[  11,  238,    9,  ...,   56,   48,   14],\n        [   3,    1,   28,  ...,    1,    1, 5033],\n        [ 327,    1,    4,  ...,    1,    1,   14],\n        ...,\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1]])\ntensor([23,  1,  6,  1, 17,  1,  9,  5,  4,  5,  3,  4,  2,  2,  1,  1,  4,  9,\n         7,  7, 23,  1,  2,  5,  1,  1,  1, 20,  1, 17,  9, 26,  1,  3,  9,  4,\n         3,  5,  1,  9,  3,  5,  1, 40, 31, 25,  9,  5,  4,  1,  3,  1, 16,  1,\n        16,  4, 23,  3,  7,  3, 11,  1,  1,  7])\ntensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1])\n-----------------------------------------------------\n","output_type":"stream"}],"id":"068e1272-6421-41c8-b39c-6e8c41abcc51"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"282b639b-0fae-4c3f-85ac-20a271b28419"},{"cell_type":"markdown","source":"## Model","metadata":{},"id":"7c2aa7a4-ae55-4277-951f-0ec321502885"},{"cell_type":"code","source":"embedding_size = 128\nhidden_size = 128\nvocab_size = len(TEXT.vocab.itos)\nnum_classes = 1\nnum_epoch= 20\n\nclass BiLSTM(nn.Module):\n    def __init__(self):\n        super(BiLSTM, self).__init__()\n        self.word_vec = nn.Embedding(vocab_size, embedding_size)\n        # bidirectional双向LSTM\n        self.bilstm = nn.LSTM(embedding_size, hidden_size, 1, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n \n    def forward(self, input):\n        embedding_input = self.word_vec(input)\n        # 调换第一维和第二维度\n        embedding_input = embedding_input.permute(1, 0, 2)\n        output, (h_n, c_n) = self.bilstm(embedding_input)\n        # 使用正向LSTM与反向LSTM最后一个输出做拼接\n        encoding1 = torch.cat([h_n[0], h_n[1]], dim=1) # dim=1代表横向拼接\n        # 使用双向LSTM的输出头尾拼接做文本分类\n        encoding2 = torch.cat([output[0], output[-1]], dim=1)\n        fc_out = self.fc(encoding1).squeeze()\n        return fc_out\n \nmodel = BiLSTM()\nprint(model)","metadata":{"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"BiLSTM(\n  (word_vec): Embedding(18003, 128)\n  (bilstm): LSTM(128, 128, bidirectional=True)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)\n","output_type":"stream"}],"id":"9d6bd6c1-de6a-4bb3-a90a-b138f60e37a6"},{"cell_type":"code","source":"class LogisticLoss(nn.Module):\n    def __init__(self):\n        super(LogisticLoss, self).__init__()\n    def forward(self, inputs, target):\n        return torch.mean(torch.log(1.0/torch.sigmoid(target * inputs)))","metadata":{"trusted":true},"execution_count":41,"outputs":[],"id":"08d02aae-b1a3-494f-a681-2351820d2a49"},{"cell_type":"code","source":"criterion = LogisticLoss()\noptimizer = torch.optim.SGD(params = model.parameters(), lr = 0.01, momentum=0.9)","metadata":{"trusted":true},"execution_count":42,"outputs":[],"id":"1b073429-70ef-4d6a-a6a4-3cc86ab705d5"},{"cell_type":"code","source":"print(len(train_iterator))\nprint(len(test_iterator))","metadata":{"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"1544\n29\n","output_type":"stream"}],"id":"4bc35a33-d25e-4b25-b478-79bdac3984f4"},{"cell_type":"code","source":"import os\nimport sys\nfname = sys.path[0] + os.sep + \"log\" + os.sep + \"LSTM_loss.txt\"\nprint(fname)","metadata":{"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"/home/jovyan/LSTM/log/LSTM_loss.txt\n","output_type":"stream"}],"id":"dd327b17-5bba-4a8e-bccc-f521846313e0"},{"cell_type":"code","source":"from torch.autograd import Variable\nwith open(fname, 'w') as f:\n    for epoch in range(num_epoch):\n        total_loss = 0\n        for i, ((encode, length), labels) in enumerate(train_iterator):\n            labels = Variable(2 * (labels.float() - 0.5))\n            pred = model(encode.transpose(0,1))\n            loss = criterion(pred, labels)\n            total_loss += loss.item()\n            if (i + 1) % 100 == 0:\n                print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n                    % (epoch + 1, num_epoch, i + 1,\n                        len(train_iterator), loss.item()))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n        # Print your results every epoch\n        epoch_loss = total_loss / len(train_iterator)\n        correct = 0\n        total = 0\n        for i, ((encode, length), labels) in enumerate(test_iterator):\n            # if i%5 == 0:\n                # print(i)\n            labels = Variable(2 * (labels.float() - 0.5))\n            outputs = model(encode.transpose(0,1))\n            predicted = torch.where(torch.sigmoid(outputs)>0.5, 1, -1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n        f.write('Epoch: [% d/% d]: Loss: %.4f , Accuracy of the currenent model: % .4f %%\\n' % (\n            epoch + 1, num_epoch,\n            epoch_loss,\n            100.0 * correct / total))\n        print('Epoch: [% d/% d]: Loss: %.4f , Accuracy of the currenent model: % .4f %%\\n' % (\n            epoch + 1, num_epoch,\n            epoch_loss,\n            100.0 * correct / total))\n        print(\"**********************************\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d5491761-c8cd-4a78-a793-bb4b8cdf537e"},{"cell_type":"code","source":"for i, ((encode, length), labels) in enumerate(train_iterator):\n    tencode = encode.transpose(0,1)\n    print(tencode)\n    print(len(tencode))\n    print(encode)\n    print(length)\n    print(labels)\n    print(\"-----------------------------------------------------\")\n    break","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor([[1852,    1,    1,  ...,    1,    1,    1],\n        [   2,   26, 2961,  ...,    1,    1,    1],\n        [1775, 6106,    1,  ...,    1,    1,    1],\n        ...,\n        [1639,    1,    1,  ...,    1,    1,    1],\n        [   6, 3690,  322,  ...,    1,    1,    1],\n        [  30,  655,    1,  ...,    1,    1,    1]])\n64\ntensor([[1852,    2, 1775,  ..., 1639,    6,   30],\n        [   1,   26, 6106,  ...,    1, 3690,  655],\n        [   1, 2961,    1,  ...,    1,  322,    1],\n        ...,\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1]])\ntensor([ 1, 14,  2,  9, 14,  6,  1,  1, 31,  9,  7, 21,  1,  3, 12,  3,  9,  1,\n         6, 35,  1,  2,  4, 22,  6, 26,  2,  1,  1,  5, 18,  1,  2, 15,  3,  4,\n         1,  8,  9,  1,  8,  1, 29,  8,  8,  2,  8,  4,  5,  5,  7, 23,  1,  9,\n         2,  3,  1, 17,  4,  3, 26,  1, 19,  2])\ntensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0])\n-----------------------------------------------------\n","output_type":"stream"}],"id":"a5e818f5-863a-411a-a2fd-d6ae1c3f459c"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"61969197-5973-4bd7-a689-121e3572949d"}]}